\documentclass[11pt]{article}
    \title{\textbf{Adversarial-resilient  Routing Graph Reconcilation for P2P networks 0.0.3}}
    \author{Piotr Mikulski}
    \date{}
\oddsidemargin=-0.2in
\topmargin=-1in
\headheight=0pt
\textwidth=6.8in
\usepackage{listings, listings-rust}
\usepackage{amsmath}

\usepackage{mathtools}
\begin{document}
\maketitle
\thispagestyle{empty}

\section{Overview}
In P2P networks, like NEAR Protocol \cite{near}, each node stores a graph of nodes and edges representing peers and connections between them.
We will call such graph a \textbf{routing graph}.
Whenever a peer joins the network or a peer reconnects, they exchange their routing tables.
The process of exchanging tables involves adding edges, which are present in one's peers view of the graph, but not in the others.
For the sake of simplicity we will describe only the process of adding new edges.
The process of adding/removing edges is described in [quotation needed].

The process of exchanging full \textbf{routing graphs} on every reconnect would be expensive and we would like to avoid it.
Ideally we would exhange the amount of information / spend computing power proportional to the symetrical difference of views of \textbf{routing tables} hold by each peer.

In this article, we will discuss practical ways of implementing \textbf{Routing Graph Reconcilation} algorithm for a P2P network by using a logarithic number of precomputed Invertible Bloom Filters (IBF)
 \cite{esrwpc}.
There are existing publications describing how to make an bloom filter resiliant to adversarial attacks \cite{adversarial}.
We will describe on how to make IBF resiliant to such attacks.

\section{Assumptions about the network}
In order to provide an accurate estimate of the solution, we will make following assumptions based on current state of NEAR Protocol \cite{near}:
\begin{itemize}
\item Each edge is represented by a vector of 360 bytes. (public keys of both peers each plus signatures)
\item It takes 2ms to verify an edge.
\item Total number of edges in the graph won't be much bigger than 1 milion.  If it was 10 million, we would be using 3.6gb of data for representing the routing table alone. It would take 5.5h for a peer to verify all egdes using a single thread.
\item Each peer has it's own view of \textbf{routing graph}. Each peer may connect to many other peers, reconnect frequently, we would like to minimize the amount of work traffic done in case in optimistic case, no or very little work needs to be done.

\end{itemize}

\section{Definitions}
\subsection{Routing Graph}
Graph, where peers are represented by nodes, and edges by connections between them.
\subsection{Node}
Each node is uniquely defined by it's public key.
\begin{lstlisting}[language=Rust]
pub struct PeerId {
    /// Peer is defined by it's public key
    public_key: PublicKey,
}
\end{lstlisting}
\subsection{Edges}
Each edge is defined by pair of peer ids and signatures from both peers.
This data structure can be as large as 360 bytes in case of NEAR Protocol network.
Therefore we would like to minimize the number of full edges we transfer.
\begin{lstlisting}[language=Rust]
pub struct Edge {
    /// Since edges are not directed `peer0 < peer1` should hold.
    pub peer0: PeerId,
    pub peer1: PeedId,
    /// Signature from parties validating the edge.
    /// These are signature of the added edge.
    signature0: Signature,
    signature1: Signature,
    /// Extra data related to removal of edges
    /// ...
}
\end{lstlisting}

\section{Bloom filters}
Bloom filter is a space-efficient probabilistic data structure introduced by Burton Homan Bloom \cite{bhb} in 1970.
It is used to test whenever an element is in the set.
\subsection{Inverse Bloom Filter (IBF)}
Invertible Bloom Filter (IBF) is a type of bloom filter, which can be used to simultaneously calculate  $D_{A-B}$ and $D_{B-A}$ using $O(d)$ space.
This data structure encodes sets in such a way that given two encodings  of two different sets, you can recover symetric difference of those two sets.
Detailed analysis can be found at ESRwPC \cite{esrwpc}.
\subsubsection{Properties}
\begin{itemize}
\item Two IBFs structures can be added/substacted from each other creating a new structure representing sum/difference of sets.
\item It can be shown that given two IBF structures of size $g$, you can recover with high probability up to $(2/3)*g$ elements if $g$ big enough. (above 1000)  \cite{esrwpc}
\item IBF is a propabilisting approach is can fail, we have to that into account. For example by sending IBF encoded with different hash function or bigger size.
\item decoding can lead to partial result. We can recover some edges even during failure.
\item IBF of size $2^k$ consists of $2^k$ boxes. Each has following structure:
\begin{itemize}
\item xor of elements stored -  let's done it's size as $s_i$
\item xor of hashes of elements stores - let's denote it's size as $h_i$
\item count - number of elements stored (optional, but allows to detect duplicates)
\end{itemize}
\item sizeof IBF with $2^k$ elements is $2^k(s_i + s_h)$, where $s_i$ is size of item, $s_h$ is size of hash.
\end{itemize}


\subsection{Adversarial-resilient Bloom filter}
Bloom filters in general are not resistent to adversarial types of attack.
There are existing publications describing how to make an bloom filter resiliant to adversarial attacks \cite{adversarial}.

\subsection{Adversarial-resilient Inverse Bloom Filter for P2P networks}
\subsubsection{Types of attacks}
If each peer uses gnerates IBF in the same way, using the same hash function.
It's possible for one of the following attacks to be used:
\begin{enumerate}
\item Hash colision of items stores in IBF -  Instead of storing whole edges directly in IBF, we can store hashes of those elements.
If hashes aren't long enough, this allow adversarial peer to create a hash collision. 
\item Set of boxes collision - Each element is added to a fixed number of boxes, it's possible to generate an edge, which gets hashed to the same set of boxes, preventing decoding of the items in that box.
\item Signature collison - If multiple items are stores inside one box and signatures are short enough it's possible to generate such signature, where hash of items stored in box would be equal to xor of hashes of that boxes. That would cause a false positive.
\end{enumerate}
\subsubsection{Approach 1 - Generate a separate IBF for each edge.}
We can generate a separate IBF for each edge.
This would add extra memory usage proportional to each connection created, and also require to compute/update such data structure for each connection.
\subsubsection{Approach 2 - introduce randomization}
We keep one data structure in memory representing set, but it would require extra computation time, and memory.
\begin{itemize}
\item To protect against attacks 1) we can increase size of item, to be large enough, what would make generating hash collisions unlikely
\item To protect against attacks 3) we can geneate longer signatures that can be generated, by computing $g$ different hash functions.
In order not to increase network bandwith, we choose random set of bits of such signature of sufficient size, which we send though the network.
\item Let's generate such set of functions $h_0, h_1, ..., h_{w_1}$ such that for every $i != j$ the following holds true $h_i(item) != h_j(item)$.
We can generate $w$ independent $IBF_i$ structures, such that each $item$ is inserted once to $IBF_i$ at position $h_i$.
Let's generate a  $IBF$, such that $IBF = IBF_i + IBF_j + IBF_k + IBF_l$ for 4 different $i,j,k,l$.
It can be be shown that there can be $\binom{w}{4}$ such IBFs generated. In order to cause a boxes colision $\binom{w}{4}$ malicious edges would have to be generated.

\end{itemize}

\section{Algorithm overview}
There are many ways in which synchronization of \textbf{routing graphs} can be done.
We have to consider a few factors: number of bytes send, number of round trips, computation time, total time.
Later I'll discuss each variant, it's strong and weak points, and one which one I think we should use.
In general the algorithm I'm proposing in as follows:
\begin{enumerate}
  \item Use a probabilisting approach to estimate the number of edges that differ. Let call it $d_e$ . (details in section~\ref{sec:estimate})
  \item Choose $k$ such that IBF with $2^k$ elements would be enough to decode $d_e$ elements.
  \item If sending computing/sending/decoding IBF of size $2^k$ is more expensive than sending all elements, send all elements and exit
  \item Otherwise send IBF of size $2^k$ if fully successful exit, otherwise increase $k$ by 1, and go back to step 3.
 If recovery was partially successful add recovered edges.
\end{enumerate}
\subsection{Analysis}
We have to send each edge in mutual difference at least once.
We have to send at least $b_o = d*s_e$ bytes, where $s_e$ is size of each edge.

Assuming we start with $k=0$, it can be proven that total number of IBF buckets sent won't be bigger than $6d$  (TODO Proof 1).
Therefore the total number of bytes won't be bigger than $6(s_e+s_h)*d$ plus metadata, $s_h$ is the size of hash.
So the total number of bytes sent won't be bigger than $6.13b_o$.
\subsection{Improvement}
We can improve the algorithm above. We know that each edge takes 360 bytes. Sending edges directly can be expensive. Instead for each edge $e$ we can compute for example 8 bytes hash $h_e$.
Instead of computing set of missing edges, we can compute set of hashes of missing edges. Given list of hashes of edges that differ, we can then send list of edges that differ.
It can be shown that the total bytes send won't be higher than $6(s_h+s_h)*d + s_e * d = (96+s_e)*d$ (TODO Proof 2). So the solution is at most $1.26$ times worse than minimum.
Using hashes could introduce potencial vurnelabilities, explanation on mitagion can be found in section~\ref{sec:mitigation}.

\subsection{Analysis of using Adversarial-resilient approach}
TODO

\section{Estimate size of difference between two sets}\label{sec:estimate}
A few variants of estimating set differences are possible.
\subsection{Don't do estimation}
It's a simple and preferred approach.
We can start with $k = 10$. It's fine to do so, because decoding $IBF$ of size $2^k$ elements takes less than 1ms, it can be represented using only 16kb. 
It can be shown that this if we had a perfect estimation we wound send only up $1.13$ bytes times the minimum instead of $1.26$.

\subsection{Strata Estimator}
Defined in \cite{esrwpc}.
This is a probabilistic estimator. If every node keeps the same Strate Estimator generated with the same hash function then it's possible to for malicious actor to generate one edge such that would cause Stata Estimator to overestimate the difference. This would cause full graph to be sent.

This could be mitigated for example by keeping a Stata Estimator computed with different hash for each connection, but this would require to use more memory.
You would need to recompute this structure on each connection.
Further reasearch is needed if that can be avoided. Though there are better solutions.
\subsection{Send hashes of random $m$ edges}
Let's say we have Alice and Bob.
They send to each other random $m$ hashes  of edges choosen with uniform distribution.
They record the number of those edges that the other side doesn't have.
Let's choose $m$ to be $2000$.
Assuming the worst case where $e = 1000000$. We would get information about random sample of $0.05$ percent of edges.
This would allow us to accurately estimate the difference as long as the $d > 1000$.
However, if $d < 1000$ then we could send start with $k = 11$ and send IBF with $2^k$ elements.
Sending $IBF$ with $k <= 11$ takes less than 1 ms, and its not expensive.

\section{Routing Graph Reconcilation Algorithm}
Let's assume Alice and Bobs are peers trying to exchange their routing tables.
\begin{lstlisting}[language=Rust]

// this is Alice code, B is Bob's peeer
// step 1
// step 1 can be merged into one rpc

// ask B for number of edges he knows about
let B_edges = B.get_num_edges();

// ask B about how many of my edges he knows about
let known_b  = B.how_many_hashes_are_know(self.get_random_e_hashes(2000));

let known_a = self.how_many_hashes_are_known(B.get_random_e_hashes(2000));

let d = self.estimate_d(known_a, known_b, self.get_num_edges(), B_edges);

// step 2
// compute k
let mut k = compute_k(d, B_edges , self.get_num_edges());
// estimate e
let e_est = B_edges  +  self.get_num_edges() - d;

// step 3
for k in 10..21 { // can be written as one round trip for every iteration
	if self.get_num_edges() == 0 || B_edges == 0 {
		// A sends all edges to B
		...
		// B sends all edges to A
		...
		break;
         }

	if k == 20 or (2^k) > e_est / 10 {
		//  A sends all hashes to B, B returns all edges unknown
		...
		//  A asks B to give all hashes known, A gets result, A send to B all edges it needs
		...
		break;
	}

	// step 4
	// ask B for IBF[k]
	let ibf_b = B.get_IBF(k);

	// try to recover
	let (success, recovered_edges) = self.get_IBF(k).recover_edges( ibf_b);

	// figure out which edges we known about
         let known_edges, unknown_edges = self.split(recovered_edges)

	// send to B missing edges
	B.send_edges( self.get_edges_from_hashes(known_edges));
	// ask B about unknown edges
	let edges_from_b = B.get_edges_by_hash(unknown_edges);

	self.add_edges(edges_from_b);

	if (success) {
		// we are done
		break;
	}

	k += 1;
}
\end{lstlisting}

\subsection{Improvements}
\begin{enumerate}
\item Estimating number of edges that differ (step 1) can be skipped to simplify the code. We can always start with k = 10 at cost of adding extra round trips in case of doing full sync.
\item The algorithm can be rewritten to reduce the number of our trips. Each iteration of loop can be made into a half of a round trip.
Details can be left as an exercise to the reader.
\item Make algorithm adversarial resilient. 
\end{enumerate}


\section{Performance results}
TODO
\subsection{adding $1000000$ edges to IBF with size of $2^{21}$}
19ms
\subsection{decoding IBF of size $2^{14}$ with 10000 edges recovered}
7ms
\subsection{decoding IBF of size $2^{18}$ with 100000 edges recovered}
70ms
\subsection{decoding IBF of size $2^{21}$ with 1000000 edges recovered}
150ms

\section{Adversarial-resilient mitiration}\label{sec:mitigation}
TODO

\section{Proofs}
TODO

\section{References}

\begin{thebibliography}{9}
\bibitem{near}
NEAR Protocol
\textit{https://near.org/}

\bibitem{esrwpc}
Efficient Set Reconciliation without Prior Context.
\textit{https://www.ics.uci.edu/~eppstein/pubs/EppGooUye-SIGCOMM-11.pdf}
\bibitem{minisketch}
Minisketch: a library for BCH-based set reconciliation
\textit{https://github.com/eupn/minisketch-rs}
\bibitem{bhb}
Space/time trade-offs in hash coding with allowable errors (1970)
\textit{https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.641.9096}
\bibitem{adversarial}
Bloom Filters in Adversarial Environments
\textit{https://eprint.iacr.org/2015/543.pdf}
\end{thebibliography}


\end{document}



\documentclass[12pt]{article}
\begin{document}

\section*{Notes for My Paper}



\section{Contributing}
If you'd like to contribute to this project, here's some ideas:
\begin{description}
\addtolength{\itemindent}{0.80cm}
\itemsep0em
\item[Development] fix bugs or add features to our C/GTK codebase
\item[Documentation] edit the user guide to improve user experience
\item[Localization] translate Gummi in your native language
\item[Testing] try out the latest and report your findings
\end{description}
Refer to the \emph{Getting Involved}\footnote{https://github.com/alexandervdm/gummi/wiki/Getting-Involved} section on our wiki for more information.


\section{What's next}
Within the 0.8.x release series we will focus on adding minor features but mostly fixing bugs. New functionality will be integrated into the next major release. An overview of currently accepted features can be found on the 0.9.0 milestone\footnote{https://github.com/alexandervdm/gummi/milestone/3} page.

\section{In closing}
We hope you will enjoy using this release as much as we enjoyed creating it. If you have any further comments, suggestions or wish to report an issue, please visit \emph{\textbf{https://gummi.app}}.


# Estimation algorithms

# Algorithm for Edges Reconciliation

# Vulnerabilities

# Results

# References
- [1] https://near.org/
- [2] Efficient Set Reconciliation without Prior Context. https://www.ics.uci.edu/~eppstein/pubs/EppGooUye-SIGCOMM-11.pdf
- [3] Minisketch: a library for BCH-based set reconciliation https://github.com/eupn/minisketch-rs
- [4] Efficient Set Reconciliation without Prior Context https://www.ics.uci.edu/~eppstein/pubs/EppGooUye-SIGCOMM-11.pdf
*/xxx
ff
\end{document}

